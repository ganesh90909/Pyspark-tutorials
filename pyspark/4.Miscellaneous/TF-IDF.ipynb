{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF, IDF, TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF**: refers to **Term Frequency** which is the frequency of a particular term in the document.Higher the TF,higher the importance of that term to the document.\n",
    "<br>**IDF**: refers to **Inverse Document Frequrncy** is the frequency of the document that contai a specific term.If a term is present in all documents,then Document frequency is amximum which is 1 and the Inverse Doc frequency is minimum.The higher the IDF, the more relevant the term is.\n",
    "<br>**TF-IDF**: refers to **Term Frequency-Inverse Document Frequency** is the product of TF and IDF.A high TF-IDF is obtained when both TF and IDF is high.(ie, frequency of term in one doc is high and frequency in all docs is low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency, HashingTF, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HashingTF**: The HashingTF() utilizes the \"*Murmurhash 3*\" function to map a raw feature (a term) into an index (a number). Hashing is the process of transforming data of arbitrary size to size-fixed, usually shorter data. The term frequencies are calculated based on the generated indices. For the HashingTF() method, the mapping process is very cheap. Because each term-to-index mapping is independent of other term-to-index mapping. The hashing function takes a unique input and gerenate a “unique result”. However, **hashing collision** may occur, which means different features (terms) may be hased to the same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer()**: It indexes terms by descending order of term frequencies in the entire corpus, NOT the term frequencies in the document. After the indexing process, the term frequencies are calculated by documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|terms                                      |\n",
      "+-------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|\n",
      "|[I, love, spark, very, very, much]         |\n",
      "|[everyone, should, use, spark]             |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'terms': [\n",
    "            ['spark', 'spark', 'spark', 'is', 'awesome', 'awesome'],\n",
    "            ['I', 'love', 'spark', 'very', 'very', 'much'],\n",
    "            ['everyone', 'should', 'use', 'spark']\n",
    "        ]\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **numFeatures** takes an integer,which should be larger than the total number of terms in the corpus and it should be power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "hashtf=HashingTF(numFeatures=pow(2,4),inputCol='terms',outputCol='features(numFeatures), [index], [term frequency]')\n",
    "\n",
    "stages=[hashtf]\n",
    "pipeline=Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|terms                                      |features(numFeatures), [index], [term frequency]|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(16,[1,15],[4.0,2.0])                           |\n",
      "|[I, love, spark, very, very, much]         |(16,[0,1,2,8,12],[1.0,1.0,1.0,2.0,1.0])         |\n",
      "|[everyone, should, use, spark]             |(16,[1,9,13],[2.0,1.0,1.0])                     |\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above,both terms 'spark' and 'is' is hashed 1 due to hashing collision.So,it has term frequency of 4.0 corresponding to three 'spark' terms and one 'is' term.The likelihood of hashing collision can be decreased by increasing the value of **numFeatures** parameter in *HashingTF* whose default value is pow(2,18)=262,144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer() has three parameters to control which terms will be kept as features:\n",
    "    <ul>\n",
    "    <li>minTF: features that has term frequency less than minTF will be removed. If minTF=1, then no features will be removed.</li>\n",
    "    <li>minDF: features that has document frequency less than minDF will be removed. If minDF=1, then no features will be removed.</li>\n",
    "    <li>vocabSize: keep terms of the top vocabSize frequencies.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "countvec=CountVectorizer(minTF=1,minDF=1,vocabSize=20, inputCol='terms', outputCol='features(vocabSize),[index], [term frequency]')\n",
    "stages=[countvec]\n",
    "pipeline=Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+---------------------------------------------+\n",
      "|terms                                      |features(vocabSize),[index], [term frequency]|\n",
      "+-------------------------------------------+---------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(10,[0,2,4],[3.0,2.0,1.0])                   |\n",
      "|[I, love, spark, very, very, much]         |(10,[0,1,3,5,7],[1.0,2.0,1.0,1.0,1.0])       |\n",
      "|[everyone, should, use, spark]             |(10,[0,6,8,9],[1.0,1.0,1.0,1.0])             |\n",
      "+-------------------------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

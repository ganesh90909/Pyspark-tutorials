{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris=spark.read.csv('Iris-data.csv',inferSchema=True,header=True)\n",
    "iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "| ID|CAPSULE|AGE|RACE|DPROS|DCAPS| PSA| VOL|GLEASON|\n",
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "|  1|      0| 65|   1|    2|    1| 1.4| 0.0|      6|\n",
      "|  2|      0| 72|   1|    3|    2| 6.7| 0.0|      7|\n",
      "|  3|      0| 70|   1|    1|    2| 4.9| 0.0|      6|\n",
      "|  4|      0| 76|   2|    2|    1|51.2|20.0|      7|\n",
      "|  5|      0| 69|   1|    1|    1|12.3|55.9|      6|\n",
      "|  6|      1| 71|   1|    3|    2| 3.3| 0.0|      8|\n",
      "|  7|      0| 68|   2|    4|    2|31.9| 0.0|      7|\n",
      "|  8|      0| 61|   2|    4|    2|66.7|27.2|      7|\n",
      "|  9|      0| 69|   1|    1|    1| 3.9|24.0|      7|\n",
      "| 10|      0| 68|   2|    1|    2|13.0| 0.0|      6|\n",
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prostate=spark.read.csv('prostate.csv',inferSchema=True,header=True)\n",
    "prostate.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivaganesh/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/shivaganesh/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| x1|\n",
      "+---+\n",
      "| 10|\n",
      "| 12|\n",
      "| -9|\n",
      "| 32|\n",
      "| -8|\n",
      "|-15|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf=pd.DataFrame({'x1':[10,12,-9,32,-8,-15]})\n",
    "df=spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| x1|abs(x1)|\n",
      "+---+-------+\n",
      "| 10|     10|\n",
      "| 12|     12|\n",
      "| -9|      9|\n",
      "| 32|     32|\n",
      "| -8|      8|\n",
      "|-15|     15|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.x1,abs(df.x1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  x1|\n",
      "+--------------------+\n",
      "|-0.45202763693223214|\n",
      "|-0.06320562523758244|\n",
      "| -0.7932405390516785|\n",
      "| 0.14741580416877742|\n",
      "|  0.2289112899417708|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf=pd.DataFrame({'x1':list(-np.random.rand(5)+list(np.random.rand(5)))})\n",
    "df=spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                  x1|          ACOS(x1)|\n",
      "+--------------------+------------------+\n",
      "|-0.45202763693223214|2.0398334856263953|\n",
      "|-0.06320562523758244|1.6340441117648803|\n",
      "| -0.7932405390516785| 2.486908910159746|\n",
      "| 0.14741580416877742|1.4228413069611212|\n",
      "|  0.2289112899417708|1.3398371980666264|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Arc Cosine (inverse of cosine) is evaluated\n",
    "df.select(df.x1,acos(df.x1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dates|\n",
      "+----------+\n",
      "|2020-05-12|\n",
      "|2020-05-13|\n",
      "|2020-05-14|\n",
      "|2020-05-15|\n",
      "|2020-05-16|\n",
      "|2020-05-17|\n",
      "|2020-05-18|\n",
      "|2020-05-19|\n",
      "|2020-05-20|\n",
      "|2020-05-21|\n",
      "|2020-05-12|\n",
      "|2020-05-13|\n",
      "|2020-05-14|\n",
      "|2020-05-15|\n",
      "|2020-05-16|\n",
      "|2020-05-17|\n",
      "|2020-05-18|\n",
      "|2020-05-19|\n",
      "|2020-05-20|\n",
      "|2020-05-21|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "base=datetime.date.today()\n",
    "date_list=[base+datetime.timedelta(days=x) for x in list(range(0,10))*10]\n",
    "pdf=pd.DataFrame({'dates':date_list})\n",
    "df=spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|     dates|add_months(dates, 2)|\n",
      "+----------+--------------------+\n",
      "|2020-05-12|          2020-07-12|\n",
      "|2020-05-13|          2020-07-13|\n",
      "|2020-05-14|          2020-07-14|\n",
      "|2020-05-15|          2020-07-15|\n",
      "|2020-05-16|          2020-07-16|\n",
      "|2020-05-17|          2020-07-17|\n",
      "|2020-05-18|          2020-07-18|\n",
      "|2020-05-19|          2020-07-19|\n",
      "|2020-05-20|          2020-07-20|\n",
      "|2020-05-21|          2020-07-21|\n",
      "|2020-05-12|          2020-07-12|\n",
      "|2020-05-13|          2020-07-13|\n",
      "|2020-05-14|          2020-07-14|\n",
      "|2020-05-15|          2020-07-15|\n",
      "|2020-05-16|          2020-07-16|\n",
      "|2020-05-17|          2020-07-17|\n",
      "|2020-05-18|          2020-07-18|\n",
      "|2020-05-19|          2020-07-19|\n",
      "|2020-05-20|          2020-07-20|\n",
      "|2020-05-21|          2020-07-21|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dates',add_months(df.dates,2)).alias('months').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|approx_count_distinct(Species)|\n",
      "+------------------------------+\n",
      "|                             3|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(approx_count_distinct(iris.Species)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    Species|            features|\n",
      "+-----------+--------------------+\n",
      "|Iris-setosa|[5.1, 3.5, 1.4, 0.2]|\n",
      "|Iris-setosa|[4.9, 3.0, 1.4, 0.2]|\n",
      "|Iris-setosa|[4.7, 3.2, 1.3, 0.2]|\n",
      "|Iris-setosa|[4.6, 3.1, 1.5, 0.2]|\n",
      "|Iris-setosa|[5.0, 3.6, 1.4, 0.2]|\n",
      "+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr=iris.select('Species',array(['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']).alias('features'))\n",
    "df_arr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|new_features|\n",
      "+--------------------+------------+\n",
      "|[5.1, 3.5, 1.4, 0.2]|        true|\n",
      "|[4.9, 3.0, 1.4, 0.2]|        true|\n",
      "|[4.7, 3.2, 1.3, 0.2]|       false|\n",
      "|[4.6, 3.1, 1.5, 0.2]|       false|\n",
      "|[5.0, 3.6, 1.4, 0.2]|        true|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr_contains=df_arr.select('features',array_contains(df_arr.features,1.4).alias('new_features'))\n",
    "df_arr_contains.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "| ID|CAPSULE|AGE|RACE|DPROS|DCAPS| PSA| VOL|GLEASON|\n",
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "| 71|      0| 68|   1|    2|    1| 0.3| 0.0|      6|\n",
      "| 49|      0| 70|   1|    2|    1| 0.4|17.1|      5|\n",
      "|357|      0| 63|   1|    1|    1| 0.5| 0.0|      0|\n",
      "| 61|      0| 59|   1|    2|    1| 0.7|96.0|      5|\n",
      "|323|      0| 63|   1|    3|    1| 0.7|18.6|      5|\n",
      "|366|      0| 55|   1|    1|    1| 0.8|21.0|      6|\n",
      "|131|      0| 73|   1|    1|    1| 1.0| 0.0|      5|\n",
      "|352|      0| 75|   1|    4|    1| 1.0|13.3|      6|\n",
      "| 37|      0| 54|   1|    2|    1| 1.0| 0.0|      6|\n",
      "|220|      0| 74|   1|    2|    1| 1.2|21.6|      6|\n",
      "|225|      0| 71|   1|    1|    1|1.29| 0.0|      7|\n",
      "| 69|      0| 65|   1|    2|    1| 1.3| 6.8|      5|\n",
      "|129|      0| 75|   1|    2|    1| 1.4| 0.0|      6|\n",
      "|277|      0| 59|   1|    2|    1| 1.4| 0.0|      6|\n",
      "|  1|      0| 65|   1|    2|    1| 1.4| 0.0|      6|\n",
      "|339|      1| 72|   1|    2|    1| 1.4|24.2|      6|\n",
      "|379|      0| 69|   2|    2|    1| 1.5| 8.6|      5|\n",
      "|210|      0| 57|   1|    3|    1| 1.5| 0.0|      5|\n",
      "|337|      0| 61|   1|    3|    1| 1.5| 0.0|      5|\n",
      "| 77|      0| 73|   1|    3|    1| 1.6|17.1|      6|\n",
      "+---+-------+---+----+-----+-----+----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#asc returns a sort expression,which can be used as an argument for pyspark.sql.DataFrame.sort() \n",
    "#or pyspark.sql.Dataframe.orderBy()\n",
    "prostate.sort(prostate.PSA.asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(PSA)|\n",
      "+------------------+\n",
      "|15.408631578947354|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prostate.select(avg('PSA')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([[1],[2],[3],[4]],['x'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|  x|current_date()|\n",
      "+---+--------------+\n",
      "|  1|    2020-05-12|\n",
      "|  2|    2020-05-12|\n",
      "|  3|    2020-05-12|\n",
      "|  4|    2020-05-12|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('x',current_date()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|x  |current_timestamp()    |\n",
      "+---+-----------------------+\n",
      "|1  |2020-05-12 19:06:16.816|\n",
      "|2  |2020-05-12 19:06:16.816|\n",
      "|3  |2020-05-12 19:06:16.816|\n",
      "|4  |2020-05-12 19:06:16.816|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('x',current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         x|\n",
      "+----------+\n",
      "|2019-12-25|\n",
      "|2019-12-26|\n",
      "| 2020-5-12|\n",
      "|  2020-3-3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([['2019-12-25'],['2019-12-26'],['2020-5-12'],['2020-3-3']],['x'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|         x|date_add(x, 6)|\n",
      "+----------+--------------+\n",
      "|2019-12-25|    2019-12-31|\n",
      "|2019-12-26|    2020-01-01|\n",
      "| 2020-5-12|    2020-05-18|\n",
      "|  2020-3-3|    2020-03-09|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('x',date_add(df.x,6)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|         x|    format|\n",
      "+----------+----------+\n",
      "|2019-12-25|2019/12/25|\n",
      "|2019-12-26|2019/12/26|\n",
      "| 2020-5-12|2020/05/12|\n",
      "|  2020-3-3|2020/03/03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('x',date_format(df.x,'YYYY/MM/dd').alias('format')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
